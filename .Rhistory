setwd("C:/Users/ivyfo/Dropbox/Master of Public Health/Master of Public Health - Courses/Fall 2018 - Courses/CHL7001 - Machine Learning/CHL7001 - Tutorials")
# Apply PCA to the iris data
#remove outcome column
iris.data <- iris[,-5]
iris.labels <- iris[,5]
#scale of features is different
apply(iris.data, 2,mean)
apply(iris.data, 2,var)
#scale the data
pr.out <- prcomp(iris.data, scale=TRUE)
pr.out
# These are the loadings, the coefficients connecting the features and the pc's 0 for pc1-pc4
pr.out$rotation
# bivariate plot, plotting together the points and the features based on the first 2 pc's
biplot(pr.out, scale=0)
pr.out$sdev
pr.var <- pr.out$sdev^2
pr.var
#pve = proportion of variance explained by each component
#variance explained by PC1 = 73%
pve <- pr.var/sum(pr.var)
pve
#x-axis should not show 1.5, 2.5, and 3.5 - not continuous - just 4 components
plot(pve, xlab="Principal Component", ylab="Proportion of
Variance Explained ", ylim=c(0,1) ,type="b")
plot(cumsum(pve), xlab="Principal Component", ylab="
Cumulative Proportion of Variance Explained ", ylim=c(0,1) ,
type="b")
#cumsum function gives cumulative sum of previous units
cumsum(pve)
plot(pr.out$x[,1:2],col = 4-as.numeric(as.factor(iris.labels)))
legend("topright",legend = levels(as.factor(iris.labels)), text.col = 4-(1:3))
boxplot(Petal.Width~Species, data=iris)
nci.labs <- NCI60$labs
nci.data <- NCI60$data
pr.out.nci <- prcomp(nci.data, scale=TRUE)
pr.var <- pr.out.nci$sdev^2
pve <- pr.var/sum(pr.var)
#scale y-axis from 0 to 1
#pc1 only explains 10% of the variance
#individuals ma look for "elbow" (big change in slope) for selection of pc - would choose 6 pc's here with this heuristic
plot(pve, xlab="Principal Component", ylab="Proportion of
Variance Explained ", ylim=c(0,1) ,type="b")
#conclusion: pca is not very useful here
#sample size of 64 is smaller than the number of features - only have 64 pca
#regression will not work - less data than covariates
plot(pve, xlab="Principal Component", ylab="Proportion of
Variance Explained ", type="b")
plot(cumsum(pve), xlab="Principal Component", ylab="
Cumulative Proportion of Variance Explained ", ylim=c(0,1) ,
type="b")
# summary contains most of this info
#after 8 principal components only 41.23% of variance explained
summary(pr.out.nci)
pr.out.nci <- prcomp(nci.data, scale=TRUE)
pr.var <- pr.out.nci$sdev^2
pve <- pr.var/sum(pr.var)
#scale y-axis from 0 to 1
#pc1 only explains 10% of the variance
#individuals ma look for "elbow" (big change in slope) for selection of pc - would choose 6 pc's here with this heuristic
plot(pve, xlab="Principal Component", ylab="Proportion of
Variance Explained ", ylim=c(0,1) ,type="b")
#conclusion: pca is not very useful here
#sample size of 64 is smaller than the number of features - only have 64 pca
#regression will not work - less data than covariates
plot(pve, xlab="Principal Component", ylab="Proportion of
Variance Explained ", type="b")
plot(cumsum(pve), xlab="Principal Component", ylab="
Cumulative Proportion of Variance Explained ", ylim=c(0,1) ,
type="b")
# summary contains most of this info
#after 8 principal components only 41.23% of variance explained
summary(pr.out.nci)
pr.out.nci <- prcomp(nci.data, scale=TRUE)
pr.var <- pr.out.nci$sdev^2
nci.labs <- NCI60$labs
library(ISLR)
library(gplots)
library(cluster)
nci.labs <- NCI60$labs
nci.data <- NCI60$data
pr.out.nci <- prcomp(nci.data, scale=TRUE)
pr.var <- pr.out.nci$sdev^2
pve <- pr.var/sum(pr.var)
#scale y-axis from 0 to 1
#pc1 only explains 10% of the variance
#individuals ma look for "elbow" (big change in slope) for selection of pc - would choose 6 pc's here with this heuristic
plot(pve, xlab="Principal Component", ylab="Proportion of
Variance Explained ", ylim=c(0,1) ,type="b")
#conclusion: pca is not very useful here
#sample size of 64 is smaller than the number of features - only have 64 pca
#regression will not work - less data than covariates
plot(pve, xlab="Principal Component", ylab="Proportion of
Variance Explained ", type="b")
plot(cumsum(pve), xlab="Principal Component", ylab="
Cumulative Proportion of Variance Explained ", ylim=c(0,1) ,
type="b")
# summary contains most of this info
#after 8 principal components only 41.23% of variance explained
summary(pr.out.nci)
# too much for the whole 6+K genes, just as an example I am selected randomly 500
#y matrix only has values of 500 randomly selected genes
y <- nci.data[,sample(1:ncol(nci.data),500)]
dimnames(y) <- list(nci.labs, paste("g", 1:ncol(y), sep=""))
pdf("heatmap.pdf")
#dendogram on left of cells
#similar cancer types on right are clustered together
#algorithm tries to cluster cells across 64 cell lines
heatmap.2(y,trace="none")
#close connection with pdf
dev.off()
#y = selected genes
#5 = number of clusters
#PAM - partitioning around medoids
pr4 <- pam(y, 5)
#use silhouette function to get sihouette width
si <- silhouette(pr4)
ssi <- summary(si)
plot(si)
# let's compare it with kmeans
#cluster function tells you which cluster each point is part of
#dist matrix is also necessary here - calculates some distance according to data
pr4 <- kmeans(y, centers=5)
si <- silhouette(pr4$cluster,dist(y))
ssi <- summary(si)
ssi #gives mean silhouette values
plot(si)
# let's investigate the number of clusters
#run PAM algorithm for different number of clusters - extract average width from silinfo$avg.width - build vector
sil_width <- c()
for(i in 1:9){
pam_fit <- pam(y, k=i+1)
# with pam fit, sil info is provided as part of the output
# for other clustering methods we would have to extract it with the
# silhouette function
sil_width[i] <- pam_fit$silinfo$avg.width
}
plot(2:10, sil_width,
xlab = "Number of clusters",
ylab = "Silhouette Width")
lines(2:10, sil_width)
# Exercise 1: let's do the same with the iris data; do they cluster better for larger k?
sil_width <- c()
for(i in 1:9){
pam_fit <- pam(iris.data, k=i+1)
# with pam fit, sil info is provided as part of the output
# for other clustering methods we would have to extract it with the
# silhouette function
sil_width[i] <- pam_fit$silinfo$avg.width
}
plot(2:10, sil_width,
xlab = "Number of clusters",
ylab = "Silhouette Width")
lines(2:10, sil_width)
load("mybabis.nonmis.RData")
mybabies.nonmis <- d
diss <- daisy(mybabies.nonmis, metric = "gower")
#diss = contains distance and dissimilarity - dissimilarity between two data points (row and column)
diss.mat <- as.matrix(diss)
dim(diss.mat)
#look at first 10x10 values - symmetrical
diss.mat[1:10,1:10]
#extract function = extract or replace the diagonal of a matrix
diag(diss.mat) <- NA
which(diss.mat < 0.01, arr.ind = T)
#these data points are very similar
mybabies.nonmis[c(620,28),]
# these ones are very dissimilar
which(diss.mat > 0.65, arr.ind = T)
#these data points are very dissimilar
mybabies.nonmis[c(832,136),]
#y = selected genes
#5 = number of clusters
#PAM - partitioning around medoids
pr4 <- pam(y, 5)
#use silhouette function to get sihouette width
si <- silhouette(pr4)
ssi <- summary(si)
plot(si)
# let's compare it with kmeans
#cluster function tells you which cluster each point is part of
#dist matrix is also necessary here - calculates some distance according to data
pr4 <- kmeans(y, centers=5)
si <- silhouette(pr4$cluster,dist(y))
ssi <- summary(si)
ssi #gives mean silhouette values
plot(si)
# let's investigate the number of clusters
#run PAM algorithm for different number of clusters - extract average width from silinfo$avg.width - build vector
sil_width <- c()
for(i in 1:9){
pam_fit <- pam(y, k=i+1)
# with pam fit, sil info is provided as part of the output
# for other clustering methods we would have to extract it with the
# silhouette function
sil_width[i] <- pam_fit$silinfo$avg.width
}
plot(2:10, sil_width,
xlab = "Number of clusters",
ylab = "Silhouette Width")
lines(2:10, sil_width)
# Exercise 1: let's do the same with the iris data; do they cluster better for larger k?
sil_width <- c()
for(i in 1:9){
pam_fit <- pam(iris.data, k=i+1)
# with pam fit, sil info is provided as part of the output
# for other clustering methods we would have to extract it with the
# silhouette function
sil_width[i] <- pam_fit$silinfo$avg.width
}
plot(2:10, sil_width,
xlab = "Number of clusters",
ylab = "Silhouette Width")
lines(2:10, sil_width)
load("mybabis.nonmis.RData")
setwd("C:/Users/ivyfo/Dropbox/Master of Public Health/Master of Public Health - Courses/Fall 2018 - Courses/CHL7001 - Machine Learning/CHL7001 - Tutorials")
pam_fit
#y = selected genes
#5 = number of clusters
#PAM - partitioning around medoids
pr4 <- pam(y, 5)
pr4
plot(pr4)
plot(pr4$cluster)
plot(pr4)
summary(pr4)
plot(pr4)
si <- silhouette(pr4$cluster,dist(y))
#y = selected genes
#5 = number of clusters
#PAM - partitioning around medoids
pr4 <- pam(y, 5)
plot(pr4$cluster)
summary(pr4)
#use silhouette function to get sihouette width
si <- silhouette(pr4)
ssi <- summary(si)
plot(si)
plot(x,pr4$cluster)
plot(y,pr4$cluster)
plot(y,col=4-pr4$cluster)
set.seed(2)
x <- matrix(rnorm(50*2), ncol=2)
x[1:25,1] <- x[1:25,1]+3
x[1:25,2] <- x[1:25 ,2] -4
plot(x)
#We now perform K-means clustering with K = 2.
km.out <- kmeans(x,2,nstart=1) #run algorithm once with one starting point, k=2
plot(x,col=4-km.out$cluster) #clusters are plotted in different colours
km.out$iter #found the solution in 1 iteration
km.out <- kmeans(x,3,nstart=1)
plot(x,col=4-km.out$cluster)
# let's redo it few times
km.out <- kmeans(x,3,nstart=1)
plot(x,col=4-km.out$cluster)
km.out <- kmeans(x,3,nstart=1)
plot(x,col=4-km.out$cluster)
tot.sse <- c()
y
plot(y)
plot(pr4$cluster)
plot(y,col=4-pr4$cluster)
# let's compare it with kmeans
#cluster function tells you which cluster each point is part of
#dist matrix is also necessary here - calculates some distance according to data
pr4 <- kmeans(y, centers=5)
plot(y,col=4-pr4$cluster)
#We now perform K-means clustering with K = 2.
km.out <- kmeans(x,2,nstart=1) #run algorithm once with one starting point, k=2
plot(x,col=4-km.out$cluster) #clusters are plotted in different colours
km.out$iter #found the solution in 1 iteration
km.out <- kmeans(x,3,nstart=1)
plot(x,col=4-km.out$cluster)
# let's redo it few times
km.out <- kmeans(x,3,nstart=1)
plot(x,col=4-km.out$cluster)
# let's compare it with kmeans
#cluster function tells you which cluster each point is part of
#dist matrix is also necessary here - calculates some distance according to data
pr4 <- kmeans(y, centers=5)
plot(y,col=4-pr4$cluster)
plot(y,col=5-pr4$cluster)
#set working directory
setwd("C:/Users/ivyfo/Dropbox/Master of Public Health/Master of Public Health - Courses/Fall 2018 - Courses/CHL7001 - Machine Learning/CHL7001 - Assignments/CHL7001 A3")
#load data and create dataset d without missing values and where V14<=1 is class 1, V14=0 is class 0
d <- read.csv("Data_Cortex_Nuclear,csv", header=T, na.strings="?") #read csv data into R, specify variable names in header, return ? for missing values
#set working directory
setwd("C:/Users/ivyfo/Dropbox/Master of Public Health/Master of Public Health - Courses/Fall 2018 - Courses/CHL7001 - Machine Learning/CHL7001 - Assignments/CHL7001 A3")
#load data and create dataset d without missing values and where V14<=1 is class 1, V14=0 is class 0
d <- read.csv("Data_Cortex_Nuclear.csv", header=T, na.strings="?") #read csv data into R, specify variable names in header, return ? for missing values
View(d)
#set working directory
setwd("C:/Users/ivyfo/Dropbox/Master of Public Health/Master of Public Health - Courses/Fall 2018 - Courses/CHL7001 - Machine Learning/CHL7001 - Assignments/CHL7001 A1")
#load package to be used
library(glmnet)
##Task 1 - Read the data into R, making sure that you code the missing values properly. The character "?" is used for denoting missing values in the .csv file. Notice that there is no header in the data file. (1 point)
bc <- read.csv("bc_data.csv", header=F, na.strings="?") #read csv data into R, specify no variable names, return ? for missing values
View(bc)
d <- na.omit(d) #only keep observations with complete information
summary(d)
